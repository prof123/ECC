

\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[dvips]{graphicx}
\usepackage{cite}
\usepackage{tikz}

\newcommand{\RM}[2]{\ensuremath{\mathcal{R}(#1,#2)}}

\newcommand{\rem}{Reed-Muller}

\newcommand{\F}{\ensuremath{\mathbb{F}}}

\newcommand{\V}[1]{\ensuremath{\mathbf{#1}}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{def}{Definition}
\newtheorem{claim}{Claim}


%opening
\title{Reed Muller Codes}
\author{Prateek Sharma}
\pagestyle{plain}


\begin{document}

\maketitle

\begin{abstract} 
This seminar report gives an introduction to the Reed-Muller family of Error correcting codes. Reed-Muller codes are one of the oldest codes used for Error correction. The construction and various interpretations of the codewords is discussed. Reed Muller codes are primarily used because of their large error correcting ability and easy decoding - hence a survey of the various decoding techniques and algorithms is presented.
\end{abstract}


\section {Introduction and Terminology}
Reed-Muller ($\mathcal{R}$) codes are are amongst the oldest and most well-known of codes. They were discovered and proposed by D. E. Muller and I. S. Reed in 1954. 
Reed-Muller codes have many interesting properties that are worth examination; they 
form an inﬁnite family of codes, and larger Reed-Muller codes can be constructed from 
smaller ones. This particular observation leads us to show that Reed-Muller codes can be 
deﬁned recursively. 
Unfortunately, Reed-Muller codes become weaker as their length increases. However, 
they are often used as building blocks in other codes. 
One of the ma jor advantages of Reed-Muller codes is their relative simplicity to encode 
messages and decode received transmissions. We examine encoding using generator matrices 
and decoding using one form of a process known as majority logic. 
Reed-Muller codes, like many other codes, have tight links to design theory; we brieﬂy 
investigate this link between Reed-Muller codes and the designs resulting from affine geometries. 

\subsection{Notation}
$\F _q$ denotes a finite field of order $q$
Throughout, unless otherwise explicity stated, $q=2$, and therefore $F={0,1}$, with the addition operation being equivalent to the boolean exclusive-or (\emph{xor}), and the multiplication operation being the boolean \emph{and}.

A vector field over $\F$, with the length of the vector being $n$ will be denoted by $F^n$. For more on vector fields and finite fields, refer to \cite{vectors}, but this very basic understanding is sufficient for the purpose of understanding this report.

We use a string of length n with elements in $\F_2$ to write a vector in the vector space $\F^2_n$ . For example, if we have the vector $\mathbf{v} = (1, 0, 1, 1, 0, 1, 0, 1) \in \F_2^8$, we simply write $\mathbf{v}$ as $10110101$. 

We shall also deal with boolean functions, and shall denote the boolean xor by $+$, instead of the customary $\oplus$ to maintain consistency with the addition operation in $F_2$. 

Three parameters of linear error-correcting codes are it's length, minimum distance, and dimension, denoted by the triplet $[n,k,d]$. The elements of a code $C$ are called the \emph{codewords}. Codewords are represented as vectors of length $n$. 
The distance between two vectors $\V{u}$ and $\V{v}$ which we are concerned about is the $Hamming Distance$, defined as:
\begin{equation}
d_H = \sum_{u_i \neq v_i} 1
\end{equation}
In other words, the hamming distance is the number of positions in which 2 vectors differ. 
Analogously, the hamming weight of a vector is defined as:
\begin{equation}
wt(x) = d(x,0) = \{ 
\begin{array}{rl} 
1 & \text{if } x \neq 0 \\
0 & \text{if } x = 0 
\end{array} \right.
\end{equation}

We will use $d(x,y) = wt(x-y)$ and $wt(x+y) = wt(x) + wt(y) -2wt(x*y)$, where $x*y = (x_1y_1, x_2y_2,\ldots,x_ny_n) .


\section {Construction}
The simplest construction of \rm is by using boolean functions.
Let $(x_1,x_2,\ldots,x_m) \in \F^m$ be the set of binary m-tuples, and let $v \equiv (x_1,x_2,\ldots,x_m)$. Consider a Boolean Function $f$, $f: \F_2^m \rightarrow {0,1} $ . Thus $f(\V{x}) = f(x_1,x_2,\ldots,x_m)$ takes an m-tuple and returns either $0$ or $1$ .

$v$ can take $2^m$ values. For each value of $v$ we compute $f(v)$. This is the familiar and ubiquitious 'Truth-table' wherein a boolean function is evaluated for all possible inputs.
In the example below, $x_1, x_2, x_3$ are rows and $f(x_1, x_2, x_3)$ is computed and represented as another row. 
\begin {center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
$x_1$ & $0$ & $0$ & $0$ & $0$ & $1$ & $1$ & $1$ & $1$ \\
$x_2$ & $0$ & $0$ & $1$ & $1$ & $0$ & $0$ & $1$ & $1$ \\
$x_3$ & $0$ & $1$ & $0$ & $1$ & $0$ & $1$ & $0$ & $1$ \\
$f$   & $0$ & $0$ & $0$ & $1$ & $1$ & $0$ & $0$ & $0$ \\

\end{tabular}
\end{center} 

Since $(x_1,x_2,\ldots,x_m)$ can take $2^m$ values, $f$ is a $n=2^m$ length vector over $F_2$. Since there are $2^2^m$ such boolean functions possible, this gives us a collection of  $2^2^m$ vectors, each of length $2^m$.
 
The Reed-Muller codes are particular subsets of this collection as described below.

Using logical operations, $f$ can be represented as a function of the $x_1, x_2, \ldots , x_m$. In the above example, for instance, $f = x_1 \vee x_2 \vee x_3 ... $. We can represent any boolean function in the Disjunctive Normal form (with or replaced by xor) \cite{Problem2}, for instance in the above example $f = $

Define a Boolean Monomial as .
The degree of the boolean monomial is ...

A Boolean polynomial is a linear combination (with coefficients in $F_2$) of boolean monomials. The degree of the boolean polynomial is the degree of the its highest term (monomial).

Therefore, the polynomials (we will drop the term boolean from here on) of degree $1$ are of the form:
\begin{equation}
 1+a_1x_1+a_2_x2+\ldots+a_nx_n
\end{equation}
The above simply represents a linear combination of $m$ vectors.
We now define the \rm codes.
\begin{equation}

\RM{r}{m} = \text{Boolean polynomials over of degree} r
\end{equation}

The boolean monomials are \begin{equation*}
1,v_1,v_2,\ldots,v_m,v_1v_2,v_1v_3,\ldots,v_1v_m,ldots,v_1v_2..v_m
\end{equations}
Thus there are \begin{equatio*}
1+\binom{m}{1}+ldots+\binom{m}{m} = 2^m 
\end{equation*} boolean monomials.

We can write the generator matrix such that the columns are the binary representations of the numbers from $0$ to $2^m -1$.

If there $f$ is an $m-ary$ function, it can take the $2^m$ input values. Hence vector $\V{f}$ has length $2^m$. Since $\V{f}$ is also a linear combination, it follows that the length of $x_1, x_2,ldots,x_m$ is $2^m$. 

Properties:
Length, $n = 2^m$
Minimum Distance, $d = 2^{m-r}$
Dimension, $k=1+..$

\subsection{Recursive Formulation}
\begin{thm}
$\RM{r+1}{m+1}$ =  \{\V{u}+\V{v}| \V{u} \in \RM{r+1}{m}, \V{v} \in \RM{r}{m}\}
\end{equation}
This is known as the concatenation construction of codes, with $+$ denoting the concatenation.
Proof:
Let $\V{f} \in \RM{r+1}{m+1}$.  $\V{f}$ can be written as \begin{equation*}
f(v_1,v_2,\ldots,v_{m+1}) = g(v_1,v_2,\ldots,v_m)+v_{m+1}h((v_1,v_2,\ldots,v_{m})
\end{equation*}
Where $g \in \RM{r+1}{m} $ and $h \in \RM{r}{m}$


\end{thm}

Theorem is also equivalent to:
\begin{equation}
G(r+1,m+1) = \begin{pmatrix}
G(r+1,m) & G(r+1,m) \\
0 & G(r,m) 
\end{pmatrix}
\end{equation}

Because of the systematic way the basis vectors are chosen, \begin{equation}
G(1,m+1) = \begin{pmatrix}
G(1,m) & G(1,m) \\
0 & 1
\end{pmatrix}
\end{equation}


Recursive definition naturally is very helpful for proving certain properties, particularly since it enables the use of induction.

\begin{thm}
Minimum distance, $d=2^{m-r}$
\begin{proof}
Proof:...
\end{proof}
\end{thm}

$\RM{0}{m} $ is the repetition code. At the other extreme $\RM{m}{m}$ is a code consisting of all possible binary sequences of length $m$.
\subsection{Properties}

The \rem codes are a nested family of codes - the codes of higher order contain those of the lower order.
\begin{thm}
\begin{equation*}
\RM{r}{m} \subset \RM{t}{m} 
\text{if} 0 \leq r \leq t \leq m \\
\end{equation*}

\begin{proof}
BF
\end{proof}
\end{thm}



\begin{thm}
$\RM{m-r-1}{m}$ is the dual code of $\RM{r}{m}$
\begin{proof}
\end{proof}
\end{thm}

\begin{thm}

\begin{proof}
\end{proof}
\end{thm}


\begin{thm}

\begin{proof}
\end{proof}
\end{thm}


\begin{thm}

\begin{proof}
\end{proof}
\end{thm}


\begin{thm}

\begin{proof}
\end{proof}
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection {Weight distribution}

\section{cyclic decoding}
In addition, it is next shown that a cyclic RM⋆ (r, m) code is simpler to decode. The 
argument is as follows. It is shown in Chapter 3 that in a cyclic code C , if (v1 , v2 , . . . , vn ) 
is a code word of C , then its cyclic shift (vn , v1 , . . . , vn 
−1 ) is also a code word of C . 
Therefore, once a particular position can be corrected with ML decoding, all the other 
positions can also be corrected with the same algorithm (or hardware circuit) by cyclically 
shifting received code words, until all n positions have been tried.
\section {Decoding}
As mentioned earlier, the RM family of codes came into prominence because of the ease of decoding. In this section we present a variety of decoding algorithms. 
Several very efficient algorithms have been known specially for the reed muller codes of the first order. 
\subsection {Majority Logic Decoding}
Majority Logic Decoding is the oldest and simplest method to decode the \rem codes. 
Some terminology is needed for a clearer understanding:
Let C be an arbitrary code. Then the \emph{parity-check} is simply a code vector in the orthogonal code $C^{T}$. 
Define the \emph{support vector} of a vector $\vec{v}$ as the coordinates where it is non-zero.For example, the support-vector of $01010100$ is $(2,4,6)$.
Suppose we are given $J$ parity checks and a coordinate position, $i$, such that the intersection of all the $J$ supports is precisely $i$. The $J$ parity checks essentially 'vote' for the position $i$ , and the result of the majority for the parity of $i$ is the value. 
A collection of such parity checks described above is said to be \emph{orthogonal} to $i$. If each of the coordinates of the code has a collection of $J$ parity checks orthogonal to it, then the code is capable of correcting upto $J/2$  errors.
What are the conditions under which a code satisfies this?
Another definition of orthogonality is: A set of parity check equations is called orthogonal on the $i$ coordinate if $x_i$ appears in each equation and no other $x_j$ appears more than once in the set.
Clearly, the minimum weight must be atleast $J+1$, because the 

\begin{Theorem} 
If there are $J$ parity checks on every co-ordinate, the code can correct $J/2$ errors.
Proof: 
Let $m_i$ be the message bit at coordinate $i$. $x_i$ is the received bit. 
Let $m_i=0$
Since there are $J$ orthogonal parity equations voting for $x_i$, if there are fewer than $J/2$ errors, then the result is unchanged. Therefore atleast $J/2$ equations are $0$ and thus $x_i=0$.
If $m_i=1$
Still less than $J/2$ equations are affected, and we get the right result.
\end{Theorem}

Note: Ties can be broken in any way consistently. Either $0$, etc.

\begin{Theorem}
The number of errors that can be corrected by one-step majority logic decoding, $E_1$ is at most $n-1/2(d'-1)$ where $d'$ is the minimum distance of the dual code.

Proof: The dual code gives the parity check equations. 
Consider the 1st coordinate, then the parity checks orthogonal will have the form
\begin{equation*}
1  1 1 1  0 0 0 0 0
1  0 0 0  1 1 1 1 1 
\end{equation*}
Since there are $J$ orthogonal equations there will be $J$ such vectors.
Therefore by the definition of orthogonality the coordinate positions in the dual code vectors cannot overlap, hence $J \leq n-1/(d'-1) $.
We get the desired result by using the previous theorem.
\end{Theorem}

\subsection{L-step decoding}
We can extend the idea of step-decoding using orthogonal parity checks by generalizing for parity check equations to be orthogonal on a set of coordinates. 

Definition: A set of parity checks $S_1, S_2,\ldots$ is orthogonal on coordinates $i,j,..$ if the sum $x_i+x_j+\ldots$ appears in each S but no other $x_p$ appears more than once in the set.

\begin{Theorem}
The number of errors which can be corrected by an L-step majority decoding scheme, $E_l \leq n/d' - 1/2 $ 


Proof:
Let the $i-th$ parity check involve $a_i$ coordinates (besides the $l$). 
Since these checks correspond to the codewords in the dual code, we have

\begin{equation}
l+a_i \geq d' 

a_i + a_j \geq d'
\end{equation}

We also have 
\begin{equation*}
S = \sum_{i=1}^{J}{a_i} \leq n-l
\end{equation*}
Thus, \begin{equation*}
Jl + S \geq Jd'

(J-1)S \geq \binom{J}{2}d'
\end{equation*}
Eliminating l and S, we get \begin{equation*}
J \leq 2n/d' -1
\end{equation*}

\end{Theorem}




\end{Theorem}
\subsection {Geometrical}
We can use the finite geometry construction of the codes to help in the decoding. 
\subsection {Hadamard Transforms}
Green machine
\subsection {List Decoding}
A list decoding procedure for a code C is a function which for given a $u\inF_q^n$ and $e > 0$ outputs all $v\elem C$ such that $\Delta(u,v) \leq e$.

Johnson Bound
\cite{zuckerman10}
If $C$ is an [n,k,d] code with $d \geq (1/2-\epsilon)$, then
\[ \forall{u}\in F_q^n | \{v \in C : \Delta(u,v) \leq (1/2 - \sqrt{\epsilon})\}| leq 1/\epsilon
\]
Proof:
(from lec10.ps)
\subsection {RM(1,m) decoding algorithm}

\section{Generalized \rm codes}

\section {Uses of Reed Muller codes}

~\cite{codingtheory+blog}
\section {References}

\bibliographystyle{plain}      
\bibliography{report}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
