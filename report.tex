
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[dvips]{graphicx}
\usepackage{cite}
\usepackage{tikz}

%%% Margins %%%
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin  0pt
\marginparsep 0pt
\topmargin   0pt

\textwidth   6.5in
\textheight  8.5 in

%%% Macros %%%
\newcommand{\RM}[2]{\ensuremath{\mathcal{R}(#1,#2)}}

\newcommand{\rem}{Reed-Muller}

\newcommand{\F}{\ensuremath{\mathbb{F}}}

\newcommand{\V}[1]{\ensuremath{\mathbf{#1}}}

%%% Theorems %%%
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
%%\newtheorem{def}{Definition}
\newtheorem{claim}{Claim}


%opening
\title{Reed-Muller Codes}
\author{Prateek Sharma}
\pagestyle{plain}

\begin{document}

\maketitle

\begin{abstract} 
This Seminar Report gives an introduction to the Reed-Muller family of Error Correcting Codes. Reed-Muller codes are one of the oldest codes used for error correction. The construction and various interpretations of the codewords is discussed. Reed Muller codes are primarily used because of their large error correcting ability and easy decoding - hence a survey of the decoding techniques and algorithms is presented. Some applications of the code are also presented. Some applications of the code to error correction and other areas are listed.
\end{abstract}


\section {Introduction}
%%swap introduction and the abstract?
Reed-Muller ($\mathcal{R}$) codes are amongst the oldest and most well-known codes. They were discovered and proposed by D. E. Muller and I. S. Reed in 1954. \cite{reed} \cite{muller}
Reed-Muller codes have many interesting properties - they can be defined recursively, and are an infinite family of codes.
Although they become weaker as their length increases, they are often used as building blocks in other codes. One of the major advantages of Reed-Muller codes is their relative simplicity to encode and decode messages. Reed-Muller codes, like many other codes, have tight links to design theory; we briefly investigate the link between Reed-Muller codes and affine geometries.


\subsection {Notation}
\label{notation}
$\F _q$ denotes a finite field of order $q$.
Throughout, unless otherwise explicity stated, $q=2$ and therefore $F=\{0,1\}$, with the addition operation being equivalent to the binary exclusive-or (\emph{xor}), and the multiplication operation being the binary \emph{and}.

A vector field over $\F$,(with the length of the vector being $n$) will be denoted by $F^n$. For more on vector fields and finite fields, refer to \cite{vectors},chapter 4 of \cite{singa} , but a very basic understanding is sufficient for understanding \rem{} codes.

We use a string of length $n$ with elements in $\F_2$ to write a vector in the vector space $\F_2^n$ . For example, if we have the vector $\mathbf{v} = (1,0,1,1,0,1,0,1) \in \F_2^8$, we simply write $\V{v}$ as $10110101$. 

We shall also deal with boolean functions, and shall denote the boolean \emph{xor} by $+$, instead of the customary $\oplus$ to maintain consistency with the addition operation in $F_2$. 

Three parameters of linear error-correcting codes are it's length, dimension, and minimum distance, denoted by the triplet $[n,k,d]$. The elements of a code $C$ are called the \emph{codewords}. Codewords are represented as vectors of length $n$.

The distance between two vectors $\V{u}$ and $\V{v}$ which we are concerned about is the \emph{Hamming Distance} which is the number of positions in which 2 vectors differ.
Analogously, the hamming weight of a vector is defined as:

\begin{equation}
wt(x) = d(x,0) = \{ 
\begin{array}{rl} 
1 & \text{if } x \neq 0 \\
0 & \text{if } x = 0 
\end{array} \right.
\end{equation}

We will use $d(x,y) = wt(x-y)$ and $wt(x+y) = wt(x) + wt(y) -2wt(x*y)$, where $x*y = (x_1y_1, x_2y_2,\ldots,x_ny_n)$ .
\label{weight-forumla}

\section {Construction of the Code}

The simplest construction of \rem{} codes is by using boolean functions.
Let $(x_1,x_2,\ldots,x_m) \in \F^m$ be the set of binary m-tuples, and let $\V{x} \equiv (x_1,x_2,\ldots,x_m)$. Consider a boolean function $f$, $f: \F^{2^m} \rightarrow \{0,1\} $ . Thus $f(\V{x}) = f(x_1,x_2,\ldots,x_m)$ takes an $m$-tuple and returns either $0$ or $1$ .

$\V{x}$ can take $2^m$ values. For each value of $\V{x}$ we compute $f(\V{x})$. This is the familiar and ubiquitious \emph{truth-table} wherein a boolean function is evaluated for all possible inputs.
In the example below, $x_1, x_2, x_3$ are rows and $f(x_1, x_2, x_3)$ is computed and represented as another row. 
\begin {center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
$x_1$ & $0$ & $0$ & $0$ & $0$ & $1$ & $1$ & $1$ & $1$ \\
$x_2$ & $0$ & $0$ & $1$ & $1$ & $0$ & $0$ & $1$ & $1$ \\
$x_3$ & $0$ & $1$ & $0$ & $1$ & $0$ & $1$ & $0$ & $1$ \\
$f$   & $0$ & $0$ & $0$ & $1$ & $1$ & $0$ & $0$ & $0$ \\

\end{tabular}
\end{center} 

Since $(x_1,x_2,\ldots,x_m)$ can take $2^m$ values, $\V{f}$ is a $n=2^m$ length vector over $F_2$. Since there are $2^2^m$ such boolean functions possible, this gives us a collection of  $2^2^m$ vectors, each of length $2^m$.
 
The Reed-Muller codes are particular subsets of this collection as described below.

$f$ can be written in the disjunctive normal form.
Using logical operations, $f$ can be represented as a function of the $x_1, x_2, \ldots , x_m$. In the above example, for instance, $f = x_1 \vee x_2 \vee x_3 ... $. We can represent any boolean function in the Disjunctive Normal form (with or replaced by xor) \cite{Problem2}, for instance in the above example $f = $

Define a Boolean Monomial as .
The monomials are \begin{equation*}
1,x_1,x_2,
\end{equation*}
There are
\begin{equation*}
  1+\binom{m}{1}+\binom{m}{2}+\ldots+\binom{m}{m} = 2^m 
\end{equation*}
distinct monomials. 
The degree of the boolean monomial is ...

A Boolean polynomial is a linear combination (with coefficients in $F_2$) of boolean monomials. The degree of the boolean polynomial is the degree of the its highest term (monomial).

Therefore, the polynomials (we will drop the term boolean from here on) of degree $1$ are of the form:
\begin{equation}
 1+a_1x_1+a_2_x2+\ldots+a_nx_n
\end{equation}
The above simply represents a linear combination of $m$ vectors.

With this, we can now define \rem{} codes of length $m$ and order $r$  as :
\begin{equation}
\RM{r}{m} = \{\V{f} : degree(f(x_1,\ldots,x_m)) = r \} 
\end{equation}
<this is not clear>

Since there are $\binom{m}{r}$ monomials of degree $r$, the number of 
The boolean monomials are \begin{equation*}
1,v_1,v_2,\ldots,v_m,v_1v_2,v_1v_3,\ldots,v_1v_m,ldots,v_1v_2..v_m
\end{equations}
Thus there are \begin{equation*}
1+\binom{m}{1}+ldots+\binom{m}{m} = 2^m 
\end{equation*} boolean monomials.

We can write the generator matrix such that the columns are the binary representations of the numbers from $0$ to $2^m -1$.

If there $f$ is an $m-ary$ function, it can take the $2^m$ input values. Hence vector $\V{f}$ has length $2^m$. Since $\V{f}$ is also a linear combination, it follows that the length of $x_1, x_2,ldots,x_m$ is $2^m$. 

\subsection{clarification}

In what follows, we frequently switch back and forth between 
B(r, {v1 , . . . , vm}) 
and 
R(r, m). Doing so in the most explicit manner would make the reasonings a 
lot harder to read, and for this reason we decided to treat codewords and Boolean 
functions as interchangeable.
In fact, boolean logic using xor is also called reed-muller logic! \cite{rmlogic}

Properties:
Length, $n = 2^m$
Minimum Distance, $d = 2^{m-r}$
Dimension, $k=1+..$

The rate of an $[n, k]$ code is deﬁned as $k/n$. 
(Thinking of the code as having $n − k$ check digits and $k$ message 
digits.

\subsection{Other interpretations}

Set theory definition can also be used to define the codes. See \cite{incidence}

The natural polynomial ring definition is standard but the boolean logic one is easier to understand.

\subsection{Code properties}

We have seen that the first order Reed-Muller code $\RM{1}{m}$ consists of all vectors $u_01 + \sum_{i=1}^m{u_i\V{v_i}} $ with $u_i=$ $0$ or $1$. 
Define the \emph{orthogonal code} $\mathcal{0}_m$ to be the $[2m, m, 2m−1 ]$ code consisting of the vectors $ \sum_{i=1}^m{u_i\V{v_i}} $
Then
\begin{equation*}
  \RM{1}{m} = \mathcal{O}_m \union (\V{1} + \mathcal{O}_m) 
\end{equation*}
 
\begin{theorem}
  Uniqueness: Any linear code with parameters $[2^m, m+1, 2^{m} ]$ is equivalent to the first order \rem{} code.
\begin{proof}[from \cite{uniq}]
  
\end{proof}
\end{theorem}

Run-length properties at \cite{syncro}


\subsection{Recursive Formulation}

\begin{thm}
$\RM{r+1}{m+1} =  \{ \V{u} | \V{u}+\V{v} : \V{u} \in \RM{r+1}{m}, \V{v} \in \RM{r}{m} \}$

This is known as the concatenation construction of codes, with $|$ denoting the concatenation.
\begin{proof} We use the boolean logic definition of the codewords.
Let $\V{f} \in \RM{r+1}{m+1}$.  $\V{f}$ can be written as \begin{equation*}
f(v_1,v_2,\ldots,v_{m+1}) = g(v_1,v_2,\ldots,v_m)+v_{m+1}h((v_1,v_2,\ldots,v_{m})
\end{equation*}
Where $\V{g} \in \RM{r+1}{m} $ and $\V{h} \in \RM{r}{m}$. Consider the associated vectors $\V{f}, \V{g'}, \V{h'}$ as polynomials over $v_1,\ldots,v_{m+1}$. 
Then, $\V{g'} = (\V{g}|\V{g})$ and $\V{h'} = (\V{0}|\V{h})$. [Problem7, \cite{sloane}. ]
Thus
\begin{equation*}
  \V{f} = \V{g}|\V{g} + \V{0}|\V{h}
\end{equation*}

\end{proof}

\end{thm}
The generator matrix version of the above theorem is

\begin{equation}
G(r+1,m+1) = \begin{pmatrix}
G(r+1,m) & G(r+1,m) \\
0 & G(r,m) 
\end{pmatrix}
\end{equation}

Because of the systematic way the basis vectors are chosen, \begin{equation}
G(1,m+1) = \begin{pmatrix}
G(1,m) & G(1,m) \\
0 & 1
\end{pmatrix}
\end{equation}


Recursive definition naturally is very helpful for proving certain properties, particularly since it enables the use of induction.

\begin{thm}
Minimum distance, $d=2^{m-r}$
\begin{proof}
Proof:...
\end{proof}
\end{thm}

$\RM{0}{m} $ is the repetition code. At the other extreme $\RM{m}{m}$ is a code consisting of all possible binary sequences of length $m$.

Another recursive construction
Let 
\begin{equation*}
R_1 = \begin{pmatrix}
1 & 1 \\
0 & 0 \\
1 & 0 \\
0 & 1 
\end{pmatrix}
\end{equation*}


Then, $R_{n+1} = \begin{pmatrix}
R_n & R_n \\
R_n & neg R_n
\end{matrix} $


\subsection{Properties}

The \rem{} codes are a nested family of codes - the codes of higher order contain those of the lower order.
\begin{thm}
\begin{equation*}
\RM{r}{m} \subset \RM{t}{m} 
\text{ if } 0 \leq r \leq t \leq m \\
\end{equation*}

\begin{proof}
By Induction.
Trivially true for $m=1$.
Let $\RM{k}{m-1} \subset \RM{l}{m-1}$ for all $0 \leq k \leq l < m$.
Let $0 <i \leq j < m$. By the recursive definition, we get:
\begin{equation*}
  \RM{i}{m}=\{(\V{u},\V{u}+\V{v} | \V{u} \in \RM{i}{m-1}, \V{v} \in \RM{i-1}{m-1} \}
\end{equation*}
Induction hypothesis gives :

\begin{equation*}
  \subset \{(\V{u},\V{u}+\V{v} | \V{u} \in \RM{j}{m-1}, \V{v} \in \RM{j-1}{m-1} \} = \RM{j}{m}
\end{equation*}

\end{proof}
\end{thm}

\begin{equation}
  \label{eq:1}
  dim(\RM{r}{m}) = dim(\RM{r}{m-1}) + dim(\RM{r-1}{m-1})
\end{equation}

\begin{thm}
  \begin{equation}
    \label{eq:2}
    
  \end{equation}
\end{thm}


\begin{thm}
$\RM{m-r-1}{m}$ is the dual code of $\RM{r}{m}$
\begin{proof}

We induct on $r$. Let $0 \leq i \leq r$.
Inductively, assume that $\RM{i}{m-1}^{\bot} = \RM{m-i-2}{m-1} $

\end{proof}
\end{thm}


\begin{thm}
  \begin{equation*}
    \RM{m-2}{m} \text{ is the extended binary hamming code }
  \end{equation*}
\begin{proof}

\end{proof}
\end{thm}

We prove a general theorem
\begin{thm}
Let $C_i$ be an $[n,k_i,d_i]$ code. Then the concatenated code defined by

  \begin{equation*}
    C = \{(\V{u},\V{u}+\V{v}) | \V{u} \in C_1 , \V{v} \in C_2 \}
  \end{equation*}

has the parameters $[2n,k_1+k_2, min{2d_1,d_2}]$ linear code.
\begin{proof}
\emph{Length:} Clearly, since length of $C_1$ and $C_2$ is $n$ each, concatenating produces vectors of length $2n$.

\emph{Dimension:} The number of words in $C$ is product of number of words in $C_1$ and $C_2$.
because $(c_1,c_2) \rightarrow (c_1,c_1+c_2) $ is a bijection. Thus, the dimension is 
\begin{equation*}
  k = k_1+k_2
\end{equation*}

\emph{Distance: } We can split this into cases, depending on whether $c_1=\V{0}$ or $c_2=\V{0}$. 

\emph{Case: \V{c_2} = \V{0}}
\begin{equation*}
  wt((c_1,c_1+c_2)) = wt(c_1,c_1) = 2wt(c_1) = 2d_1
\end{equation*}

\emph{Case: \V{c_2} \neq \V{0}}
\begin{equation*}
   wt((c_1,c_1+c_2)) = wt(c_1) + wt(c_1+c_2) \geq wt(c_1) + wt(c_2)-wt(c_1) 
= wt(c_2) = d_2
\end{equation*}


Thus, the minimum distance of the code, which is equal to the weight of the minimum weight vector is $ min{2d_1,d_2}$

\end{proof}
\end{thm}


\begin{lem}
Every codeword in $\RM{1}{m}$ (except $\V{1}, \V{0}$) has weight $2^{m-1}$
\begin{proof}
This can also be proved by using the randomization lemma for boolean functions as done in \cite{sloane}. However, if we notice that
\begin{equation*}
  \RM{1}{m} = \{(u,u) : u \in \RM{1}{m-1} \} \union \{(u,\vec{1}+u) : u \in \RM{1}{m-1} \} 
\end{equation*}
We use induction on $m$ and are done.
\end{proof}
\end{lem}

\begin{thm}
  \begin{equation*}
    \RM{m-r-1}{m} = \RM{r}{m}^{\bot}
  \end{equation*}
  \begin{proof}
    Let $\V{a} \in \RM{m-r-1}{m}$, and $\V{b} \in \RM{r}{m}$.
Then $a$ and $b$ are polynomials of degree $m-r-1$ and $r$ respectively.
$\V{ab} \in \RM{m-2}{m}$, and since $\RM{m-2}{m}$ contains all the even weight vectors, $a\cdot b \equiv 0 (mod 2) $.
Thus, $\RM{m-r-1}{m} \subset \RM{r}{m}^{\bot}$.
But by considering the dimensions we get
\begin{equation*}
  dim(\RM{m-r-1}{m}) + dim(\RM{r}{m} = 2^m = n
\end{equation*}
This implies the relation.
  \end{proof}
\end{thm}

\begin{thm}
The dual code $\RM{1}{m}^{\bot}$ is the extended binary Hamming code $H(m)$ 
  \begin{proof}
    TODO. this is important. From singa book
  \end{proof}
\end{thm}


\section{Bounds}

$A_q(n, d)$ is the largest integer $M$ such that there exists a 
$q$-ary $(n, M, d)$ code.
A $q$-ary $(n, M, d)$ code $C$ is called optimal if $M = A_q(n, d)$. 

The plotkin bound shows that the codes are all optimal.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Weight distribution}

Stated without proof
\cite{kasami+tokura}
\begin{equation}
  \label{eq:4}
  \text{There are at least} 2^{mr−r(r−1)} . 
\text{minimum weight codewords in } \RM{r}{m}. 

\end{equation}

\section{cyclic decoding}

In addition, it is next shown that a cyclic RM⋆ (r, m) code is simpler to decode. The 
argument is as follows. It is shown in Chapter 3 that in a cyclic code C , if (v1 , v2 , . . . , vn ) 
is a code word of C , then its cyclic shift (vn , v1 , . . . , vn 
−1 ) is also a code word of C . 
Therefore, once a particular position can be corrected with ML decoding, all the other 
positions can also be corrected with the same algorithm (or hardware circuit) by cyclically 
shifting received code words, until all n positions have been tried.

\RM{}{}

\section {Decoding}

As mentioned earlier, the RM family of codes came into prominence because of the ease of decoding. In this section we present a variety of decoding algorithms. 
Several very efficient algorithms have been known specially for the \rem{} codes of the first order. 

\subsection {Majority Logic Decoding}

Majority Logic Decoding is the oldest and simplest method to decode the \rem{} codes. \cite{assmus}
Some terminology is needed for a clearer understanding:
Let C be an arbitrary code. Then the \emph{parity-check} is simply a code vector in the orthogonal code $C^{\bot}$. 
Define the \emph{support vector} of a vector $\vec{v}$ as the coordinates where it is non-zero.For example, the support-vector of $01010100$ is $(2,4,6)$.
Suppose we are given $J$ parity checks and a coordinate position, $i$, such that the intersection of all the $J$ supports is precisely $i$. The $J$ parity checks essentially 'vote' for the position $i$ , and the result of the majority for the parity of $i$ is the value. 
A collection of such parity checks described above is said to be \emph{orthogonal} to $i$. If each of the coordinates of the code has a collection of $J$ parity checks orthogonal to it, then the code is capable of correcting upto $J/2$  errors.
What are the conditions under which a code satisfies this?
Another definition of orthogonality is: A set of parity check equations is called orthogonal on the $i$ coordinate if $x_i$ appears in each equation and no other $x_j$ appears more than once in the set.
Clearly, the minimum weight must be atleast $J+1$, because the 

\begin{Theorem} 
If there are $J$ parity checks on every co-ordinate, the code can correct $J/2$ errors.
Proof: 
Let $m_i$ be the message bit at coordinate $i$. $x_i$ is the received bit. 
Let $m_i=0$
Since there are $J$ orthogonal parity equations voting for $x_i$, if there are fewer than $J/2$ errors, then the result is unchanged. Therefore atleast $J/2$ equations are $0$ and thus $x_i=0$.
If $m_i=1$
Still less than $J/2$ equations are affected, and we get the right result.
\end{Theorem}

Note: Ties can be broken in any way consistently. Either $0$, etc.

\begin{Theorem}
The number of errors that can be corrected by one-step majority logic decoding, $E_1$ is at most $n-1/2(d'-1)$ where $d'$ is the minimum distance of the dual code.

Proof: The dual code gives the parity check equations. 
Consider the 1st coordinate, then the parity checks orthogonal will have the form
\begin{equation*}
1  1 1 1  0 0 0 0 0
1  0 0 0  1 1 1 1 1 
\end{equation*}
Since there are $J$ orthogonal equations there will be $J$ such vectors.
Therefore by the definition of orthogonality the coordinate positions in the dual code vectors cannot overlap, hence $J \leq n-1/(d'-1) $.
We get the desired result by using the previous theorem.
\end{Theorem}


\subsection{L-step decoding}

We can extend the idea of step-decoding using orthogonal parity checks by generalizing for parity check equations to be orthogonal on a set of coordinates. 

Definition: A set of parity checks $S_1, S_2,\ldots$ is orthogonal on coordinates $i,j,..$ if the sum $x_i+x_j+\ldots$ appears in each S but no other $x_p$ appears more than once in the set.

\begin{Theorem}
The number of errors which can be corrected by an L-step majority decoding scheme, $E_l \leq n/d' - 1/2 $ 


Proof:
Let the $i-th$ parity check involve $a_i$ coordinates (besides the $l$). 
Since these checks correspond to the codewords in the dual code, we have

\begin{equation}
l+a_i \geq d' 

a_i + a_j \geq d'
\end{equation}

We also have 
\begin{equation*}
S = \sum_{i=1}^{J}{a_i} \leq n-l
\end{equation*}
Thus, \begin{equation*}
Jl + S \geq Jd'

(J-1)S \geq \binom{J}{2}d'
\end{equation*}
Eliminating l and S, we get \begin{equation*}
J \leq 2n/d' -1
\end{equation*}

\end{Theorem}


\subsection {Reed Decoding Algorithm}

The Reed decoding algorithm is a majority logic decoding scheme for decoding \rem{} codes of any order.

Each codeword from $\RM{r}{m}$ is an incidence vector that deﬁnes a subspace in 
, in which the nonzero coordinates of the codeword are points lying in the 
subspace. Based upon this geometry we can ﬁnd equations for orthogonal checksums as follows. To ﬁnd a set of orthogonal checksums that provide an estimate for the kth order message bit $m_{i_1,i_2 ,... ,i_k} corresponding to the basis vectors v_{i_1}v_{i_{2}}\ldots v_i_k$. 

Let the corresponding subspace be $S$.
Let $T$ be the complementary subspace of $S$ where $j_1j_2\ldotsj_{m-k} = {1,2,ldots,m}-{i_1,i_2,\ldots,i_k}.$ The incidence vector of $T$ is v. <todo>

Define a \emph{Translate} of a subspace with respect to a point to be the subspace obtained by adding the binary representation of the point to each element of the subspace. 

\begin{them}
  If there are no errors, the message bit $m_b$ is given by 
  \begin{equation*}
    m_b = \sum_{P \in U_i}{x_P} \quad i=1,2,\ldots,2^{m-r}
  \end{equation*}
  \begin{proof}[geometry]
    
  \end{proof}
\end{them}

For a detailed algorithm refer to \cite{cooke} and \cite{lec9}

\subsection {Geometrical}

We can use the finite geometry construction of the codes to help in the decoding. 

\subsection {Hadamard Transforms}

\begin{def}
  Real Vector $\V{F}$ of a binary vector $\V{V}$ is a vector with $0$ replaced by $1$ and $1$ by $-1$.
\end{def}
\begin{lem}
  The Orthogonal code $\mathcal{O}_m$ with real vectors is equivalent to the Hadamard matrix $H_m$.
\begin{proof}
  Let the elements of this real-vectorized orthogonal code be $v$. 
  \begin{claim}
    \begin{equation*}
      v_i, v_j \in \RM{1}{m} \implies v_i\cdot v_j = 0
    \end{equation*}
    \begin{proof}
      The systematic choice of the basis vectors helps. We use the recursive definition of the Generator matrix.

    \end{proof}
  \end{claim}
This is the definition of the hadamard matrix too. 
\end{proof}
\end{lem}

\begin{equation}
  \label{eq:3}
  \RM{1}{m} =
  \begin{pmatrix}
    H_m \\
    -H_m
  \end{pmatrix}
\end{equation}

Keeping note that the code has been converted to the real-vector form, maximizing $FH_m$ is equivalent to finding the least-distance neighbour to $\V{v}$.
This would entail doing a simple vector product with each row of the hadamard matrix. There are $n$ such rows, and we need $O(n^2)$ operations. However, by using the \emph{Fast Hadamard Transform }, we can speed it up to $O(nlogn)$.

This is the fastest classical decoding technique and was employed in the \emph{Green Machine} decoder for the Mariner-9 space mission. 

\subsection {List Decoding}

A list decoding procedure for a code $C$ is a function which for given a $\V{u} \in F_q^n$ and $e > 0$ outputs all $v \elem C$ such that $\d(u,v) \leq e$. Alternately, given a decoding radius $T$, it should produce a list (set) of codewords which are distance less than T. List decoding is an old technique given by P.Elias in 1950 \cite{elias}, but has recently found significant attention.

The existance of a good, fast list decoding algorithm for the first order \rem{} codes was given by goldreich and levin \cite{goldreich}, \cite{sudan}. However they do not give a usable algorithm. Recently, Kabatiansky, Dumer and Tavernier \cite{kabatiansky}, \cite{dumer} have presented a simple list decoding algorithm for $\RM{1}{m}$ capable of correcting $n(\frac{1}{2} - \epsilon)$ errors in $O(n \epsilon^3)$. This is significantly better than the $\frac{n}{4}$ correcting capability of the Hadamard transform \ref{}, which uses $O(nlogn)$ time.

Let $\V{y}$ be a received vector and $L_{\epsilon}(y) = 
\{f \in \RM{1}{m} : d(\V{y}, \V{f} ) \leq n(\frac{1}{2} - \epsilon) \} $
 be the desired list. The algorithm works recursively by ﬁnding on the $i$-th step a list $L_\epsilon^i (y)$ of `candidates' which should (but may not) coincide with $i$-preﬁx of some $f(x1 , ..., xm) = f0 + f1 x1 + . . . + fmxm ∈ Lϵ (y)$ 

The main idea is to approximate the Hamming distance between the received vector y and an arbitrary ``propagation'' of a candidate $c(i) (x1 , . . . , xm) = c1 x1 + . . . + ci xi$ by the sum of Hamming distances over all $i$-dimensional ``facets'' of the $m$-dimensional Boolean cube.

The algorithm  



Johnson Bound
\cite{zuckerman10}
If $C$ is an [n,k,d] code with $d \geq (1/2-\epsilon)$, then
\[ \forall{u}\in F_q^n | \{v \in C : \Delta(u,v) \leq (1/2 - \sqrt{\epsilon})\}| leq 1/\epsilon
\]
Proof:
(from lec10.ps)

\subsection {RM(1,m) decoding algorithm}

kabatiansky


\section{Generalized \rm codes}

\section {Applications}

\subsection{Communication}

The oldest and original use. \rem{} codes were used in the 1969 Mariner-9 deep space probe to the IEEE 802.11b standard for Wireless Local Area Networks (WLANs) \cite{feldman}.         

\subsection{Tesing Low-degree polynomials}

\cite{lowdeg} 
Using $\RM{1}{m}$ codes to test whether a binary function is a low-degree polynomial. The functions are mapped to the \rem{} codes, and the properties are used to prove the bounds on the number of queries needed.

\subsection{Sidenkov cryptanalysis}

\cite{sidenkov} A variant of the McEliece cryptosystem using \rem{} codes instead of Goppa codes , called the Sidenkov system, because of the efficient decoding.
A cryptanalysis attack also uses the properties of \rem{} codes to break the cryptographic code. The uniqueness result proved earlier is a central feature in the cryptographic attack.

\subsection{Side Channel attacks}

\cite{roche}

~\cite{codingtheory+blog}
\section {References}

\bibliographystyle{plain}      
\bibliography{report}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
