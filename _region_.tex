\message{ !name(report.tex)}
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[dvips]{graphicx}
\usepackage{cite}
\usepackage{tikz}

%%% Margins %%%
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin  0pt
\marginparsep 0pt
\topmargin   0pt

\textwidth   6.5in
\textheight  8.5 in

%%% Macros %%%
\newcommand{\RM}[2]{\ensuremath{\mathcal{R}(#1,#2)}}

\newcommand{\rem}{Reed-Muller}

\newcommand{\F}{\ensuremath{\mathbb{F}}}

\newcommand{\V}[1]{\ensuremath{\mathbf{#1}}}

\newcommand{\slm}{Sloane MacWilliams}

%%% Theorems %%%
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
%%\newtheorem{def}{Definition}
\newtheorem{claim}{Claim}


%opening
\title{Reed-Muller Codes}
\author{Prateek Sharma}
\pagestyle{plain}

\begin{document}

\message{ !name(report.tex) !offset(-3) }


\maketitle

\begin{abstract} 
This Seminar Report gives an introduction to the Reed-Muller family of Error Correcting Codes. The construction and various interpretations of the codewords is discussed. Reed Muller codes are primarily used because of their large error correcting ability and easy decoding - hence a survey of the decoding techniques and algorithms is presented. Some applications of the code are also presented. Some applications of the code to error correction and other areas are listed.
\end{abstract}


\section {Introduction}

%% OK

Reed-Muller ($\mathcal{R}$) codes are amongst the oldest and most well-known codes. They were discovered by D. E. Muller and I. S. Reed in 1954. \cite{reed} \cite{muller}
Reed-Muller codes have many interesting properties - they can be defined recursively, and are an infinite family of codes. The recursive definitions are studied in section ~\ref{recusive}.
Although they become weaker as their length increases, they are often used as building blocks in other codes. One of the major advantages of Reed-Muller codes is their relative simplicity to encode and decode messages. Several decoding algorithms are presented in \ref{decoding}.  Reed-Muller codes, like many other codes, have tight links to design theory; we briefly investigate the link between Reed-Muller codes and affine geometries in \ref{geometry}.
Most famously used to transmit live images of mars in the Mariner-9 mission in 1972, ~\cite{space},~\cite{sloane} , they have applications in a wide variety of areas- some of which are presented in ~\ref{applications}.


\subsection {Notation}
\label{notation}

%% OK

$\F{} _q$ denotes a finite field of order $q$.
Throughout, unless otherwise explicity stated, $q=2$ and therefore $F=\{0,1\}$, with the addition operation being equivalent to the binary exclusive-or (\emph{xor}), and the multiplication operation being the binary \emph{and}.

A vector field over $\F{}$,(with the length of the vector being $n$) will be denoted by $F^n$.
%% For more on vector fields and finite fields, refer to \cite{vectors},chapter 4 of \cite{singa} , but a very basic understanding is sufficient for understanding \rem{} codes.

We use a string of length $n$ with elements in $\F{}_2$ to write a vector in the vector space $\F_2^n$ . For example, if we have the vector $\mathbf{v} = (1,0,1,1,0,1,0,1) \in \F_2^8$, we simply write $\V{v}$ as $10110101$. 

We shall also deal with boolean functions, and shall denote the boolean \emph{xor} by $+$, instead of the customary $\oplus$ to maintain consistency with the addition operation in $F_2$. 

Three parameters of linear error-correcting codes are it's length, dimension, and minimum distance, denoted by the triplet $[n,k,d]$. The elements of a code $C$ are called the \emph{codewords}. Codewords are represented as vectors of length $n$.

The distance between two vectors $\V{u}$ and $\V{v}$ which we are concerned about is the \emph{Hamming-Distance} which is the number of positions in which 2 vectors differ.
Analogously, the Hamming weight of a vector is defined as:
$$ $$
\begin{equation}
wt(x) = d(x,0) = \left{ 
\begin{array}{rl} 
1 & \text{if } x \neq 0 \\
0 & \text{if } x = 0 
\end{array} \right.
\end{equation}

We will use $d(x,y) = wt(x-y)$ and $wt(x+y) = wt(x) + wt(y) -2wt(x*y)$, where $x*y = (x_1y_1, x_2y_2,\ldots,x_ny_n)$ .
\label{weight-forumla}

\section {Construction of the Code}

The simplest construction of \rem{} codes is by using boolean functions.
Let $(x_1,x_2,\ldots,x_m) \in \F^m$ be the set of binary m-tuples, and let $\V{x} \equiv (x_1,x_2,\ldots,x_m)$. Consider a boolean function $f$, $f: \F^{2^m} \rightarrow \{0,1\} $ . Thus $f(\V{x}) = f(x_1,x_2,\ldots,x_m)$ takes a binary $m$-tuple and returns either $0$ or $1$ .

$\V{x}$ can take $2^m$ values. For each value of $\V{x}$ we compute $f(\V{x})$. This is the familiar and ubiquitous \emph{truth-table} wherein a boolean function is evaluated for all possible inputs.
In the example below, $x_1, x_2, x_3$ are rows and $f(x_1, x_2, x_3)$ is computed and represented as another row. 
\begin {center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
$x_1$ & $0$ & $0$ & $0$ & $0$ & $1$ & $1$ & $1$ & $1$ \\
$x_2$ & $0$ & $0$ & $1$ & $1$ & $0$ & $0$ & $1$ & $1$ \\
$x_3$ & $0$ & $1$ & $0$ & $1$ & $0$ & $1$ & $0$ & $1$ \\
$f$   & $0$ & $1$ & $0$ & $1$ & $0$ & $1$ & $1$ & $0$ \\

\end{tabular}
\end{center} 

Since $(x_1,x_2,\ldots,x_m)$ can take $2^m$ values, $\V{f}$ is a $n=2^m$ length vector over $F_2$. Since there are $2^2^m$ such boolean functions possible, this gives us a collection of  $2^2^m$ vectors, each of length $2^m$.
 
The Reed-Muller codes are particular subsets of this collection as described below.

$f$ can be written in the disjunctive normal form.
Using logical operations, $f$ can be represented as a function of the $x_1, x_2,\ldots , x_m$. In the above example, for instance, $f = x_3 + x_1x_2$. 

If $f$ is an $m-ary$ function, it can take the $2^m$ input values. Hence $\V{f}$ and $x_1,x_2,\ldots \x_m$ have length $2^m$. Since $\V{f}$ is also a linear combination, it follows that the length of $x_1, x_2,ldots,x_m$ is $2^m$. 

Let the set of boolean monomials be:
\begin{equation*}
M = \{1,x_1,x_2,\ldots,x_m,x_1x_2,\ldots,x_{m-1}x_m,x_1x_2x_3,\ldots,x_1x_2\ldots x_m\}
\end{equation*}
Where $\V{1}$ is the all ones vector.

There are
\begin{equation*}
  1+\binom{m}{1}+\binom{m}{2}+\ldots+\binom{m}{m} = 2^m 
\end{equation*}
distinct monomials. 

A Boolean polynomial is a linear combination (with coefficients in $F_2$) of boolean monomials. The degree of the boolean polynomial is the degree of the its highest term (monomial).
We can represent any Boolean function in the Disjunctive Normal Form (with or replaced by xor), and represent the Boolean function $f$ as :
\begin{equation*}
  f = 1 + a_1x_1+a_2x_2+\ldots+a_mx_m + a_{12}x_1x_2+\ldots+a_{12\ldots r}x_1x_2\ldots x_r+\ldots
\end{equation*}

\footnote{The interchange between Boolean functions and their corresponding vector representation can get confusing at times.In fact, Boolean logic using exclusive-or is also called Reed-Muller logic \cite{rm-logic}}

The polynomials (we will drop the term Boolean from here on) of degree $1$ are simply the linear combination of $m$ vectors.
\begin{equation}
 \V{1}+a_1x_1+a_2x_2+\ldots+a_mx_m
\end{equation}
The above 

With this, we can now define \rem{} codes of length $m$ and order $r$  as :
\begin{equation}
\RM{r}{m} = \{\V{f} : degree(f(x_1,\ldots,x_m)) = r \} 
\end{equation}


\begin{lemma}
  $\RM{r}{m}$ is a linear code.
  \begin{proof}
As defined, $\RM{r}{m}$ is a linear combination of the monomials of degree $\leq r$. 
The vectors $x_1,x_2,\ldots,x_m$ are clearly linearly independent.
Monomials of higher degree are composed by component-wise multiplication of the above vectors, and hence are also linearly independent.
Thus the boolean monomial set $M$ consists of linearly independent vectors, and thus any subset of that is also independent. 
  \end{proof}
\end{lemma}

From the above proof:
The monomials of degree $\leq r$ thus form a basis for $\RM{r}{m}$. The number of monomials of degree $\leq r$ is thus equal to the dimension ($k$) of $\RM{r}{m}$.
\begin{equation}
  \label{eq:2}
  \text{ The dimension } of $\RM{r}{m}$ , k= 1+\binom{m}{1}+\binom{m}{2}+\ldots+\binom{m}{r}
\end{equation}

Let $G(r,m)$ denote the \emph{Generator Matrix} of $\RM{r}{m}$. It consists of all the monomials from set $M$ of degree $\leq r$.
\begin{equation}
  \label{eq:5}
  G(r,m) =
  \begin{pmatrix}
    \V{1} \\
    \V{x_1} \\
    \V{x_2} \\
    \vdots \\
    \V{x_m} \\
    \V{x_1x_2} \\
    \vdots \\
    \V{x_1x_2\ldots x_r}
  \end{pmatrix}
\end{equation}

$\RM{0}{m} $ is the repetition code ($2^m$ repetition). At the other extreme $\RM{m}{m}$ is a code consisting of all possible binary sequences of length $2^m$.


\begin{tabular}[center]{|c|c|}
\hline
Length & $n = 2^m$ \\
Minimum Distance & $d = 2^{m-r}$ \\
Dimension & $k= 1+\binom{m}{1}+\binom{m}{2}+\ldots+\binom{m}{r}$ \\
\hline
\end{tabular}

Thus, $\RM{r}{m}$ is an $[2^m,1+\binom{m}{1}+\binom{m}{2}+\ldots+\binom{m}{r}, 2^{m-r}]$ linear code.

%%move up?
The rate of an $[n, k]$ code is deﬁned as $k/n$. 
(Thinking of the code as having $n − k$ check digits and $k$ message 
digits.


\subsection{Code properties}
\label{properies}

We have seen that the first order Reed-Muller code $\RM{1}{m}$ consists of all vectors $u_0\V{1} + \sum_{i=1}^m{u_i\V{v_i}} $ with $u_i=$ $0$ or $1$. 
Define the \emph{orthogonal code} $\mathcal{0}_m$ to be the $[2m, m, 2m−1]$ code consisting of the vectors $ \sum_{i=1}^m{u_i\V{v_i}} $
Then
\begin{theorem}
  \begin{equation*}
  \RM{1}{m} = \mathcal{O}_m \union (\V{1} + \mathcal{O}_m) 
\end{equation*}
\begin{proof}
  The definition of the orthogonal codes is exactly similar to the corresponding \rem{} codes, with only the complements of the codewords missing. 
\end{proof}
\end{theorem}

 
\begin{theorem}
  Uniqueness: Any linear code with parameters $[2^m, m+1, 2^{m} ]$ is equivalent to the first order \rem{} code.
\begin{proof}[from \cite{uniq}]
  \emph{DO THIS PROOF}.
\end{proof}
\end{theorem}

Run-length properties at \cite{runlen}


\subsection{Recursive Formulation}
\label{recursive}
\begin{thm}
$\RM{r+1}{m+1} =  \{ \V{u} | \V{u}+\V{v} : \V{u} \in \RM{r+1}{m}, \V{v} \in \RM{r}{m} \}$


This is known as the concatenation construction of codes, with $|$ denoting the concatenation.

\begin{proof} We use the boolean logic definition of the codewords.
Let $\V{f} \in \RM{r+1}{m+1}$.  $\V{f}$ can be written as \begin{equation*}
f(v_1,v_2,\ldots,v_{m+1}) = g(v_1,v_2,\ldots,v_m)+v_{m+1}h((v_1,v_2,\ldots,v_{m})
\end{equation*}
Where $\V{g} \in \RM{r+1}{m} $ and $\V{h} \in \RM{r}{m}$. Consider the associated vectors $\V{f}, \V{g'}, \V{h'}$ as polynomials over $v_1,\ldots,v_{m+1}$. 
Then, $\V{g'} = (\V{g}|\V{g})$ and $\V{h'} = (\V{0}|\V{h})$. [Problem7, \cite{sloane}. ]
Thus
\begin{equation*}
  \V{f} = \V{g}|\V{g} + \V{0}|\V{h}
\end{equation*}
Thus we have shown that the vector in $\RM{r+1}{m+1}$ can be decomposed into vectors of $\RM{r+1}{m}$ and $\RM{r}{m}$.
\end{proof}

\end{thm}
The generator matrix version of the above theorem is

\begin{equation}
G(r+1,m+1) = \begin{pmatrix}
G(r+1,m) & G(r+1,m) \\
0 & G(r,m) 
\end{pmatrix}
\end{equation}

We can also choose the vectors of the generator matrix in a systematic way \begin{equation}
G(1,m+1) = \begin{pmatrix}
G(1,m) & G(1,m) \\
0 & 1
\end{pmatrix}
\end{equation}
This way, the columns of $G(1,m)$ are binary representations of numbers from $1 to 2*^m$ in descending order.




Another recursive construction:
Let 
\begin{equation*}
R_1 = \begin{pmatrix}
1 & 1 \\
0 & 0 \\
1 & 0 \\
0 & 1 
\end{pmatrix}
\end{equation*}


Then, $R_{n+1} = \begin{pmatrix}
R_n & R_n \\
R_n & neg R_n
\end{matrix} $


\subsection{Properties}
\label{properties}

Recursive definition naturally is very helpful for proving certain properties, particularly since it enables the use of induction.


The \rem{} codes are a nested family of codes - the codes of higher order contain those of the lower order.
\begin{thm}
\begin{equation*}
\RM{r}{m} \subset \RM{t}{m} 
\text{ if } 0 \leq r \leq t \leq m \\
\end{equation*}

\begin{proof}
By Induction.
Trivially true for $m=1$.
Let $\RM{k}{m-1} \subset \RM{l}{m-1}$ for all $0 \leq k \leq l < m$.
Let $0 <i \leq j < m$. By the recursive definition, we get:
\begin{equation*}
  \RM{i}{m}=\{(\V{u},\V{u}+\V{v} | \V{u} \in \RM{i}{m-1}, \V{v} \in \RM{i-1}{m-1} \}
\end{equation*}
Induction hypothesis gives :

\begin{equation*}
  \subset \{(\V{u},\V{u}+\V{v} | \V{u} \in \RM{j}{m-1}, \V{v} \in \RM{j-1}{m-1} \} = \RM{j}{m}
\end{equation*}

\end{proof}
\end{thm}

\begin{equation}
  \label{eq:1}
  dim(\RM{r}{m}) = dim(\RM{r}{m-1}) + dim(\RM{r-1}{m-1})
\end{equation}




\begin{thm}
  \begin{equation*}
    \RM{m-2}{m} \text{ is the extended binary hamming code }
  \end{equation*}
\begin{proof}

\end{proof}
\end{thm}

We prove a general theorem
\begin{thm}
\label{general}
Let $C_i$ be an $[n,k_i,d_i]$ code. Then the concatenated code defined by

  \begin{equation*}
    C = \{(\V{u},\V{u}+\V{v}) | \V{u} \in C_1 , \V{v} \in C_2 \}
  \end{equation*}

has the parameters $[2n,k_1+k_2, min{2d_1,d_2}]$ linear code.
\begin{proof}
\emph{Length:} Clearly, since length of $C_1$ and $C_2$ is $n$ each, concatenating produces vectors of length $2n$.

\emph{Dimension:} The number of words in $C$ is product of number of words in $C_1$ and $C_2$.
because $(c_1,c_2) \rightarrow (c_1,c_1+c_2) $ is a bijection. Thus, the dimension is 
\begin{equation*}
  k = k_1+k_2
\end{equation*}

\emph{Distance: } We can split this into cases, depending on whether $c_1=\V{0}$ or $c_2=\V{0}$. 

\emph{Case: \V{c_2} = \V{0}}
\begin{equation*}
  wt((c_1,c_1+c_2)) = wt(c_1,c_1) = 2wt(c_1) = 2d_1
\end{equation*}

\emph{Case: \V{c_2} \neq \V{0}}
\begin{equation*}
   wt((c_1,c_1+c_2)) = wt(c_1) + wt(c_1+c_2) \geq wt(c_1) + wt(c_2)-wt(c_1) 
= wt(c_2) = d_2
\end{equation*}


Thus, the minimum distance of the code, which is equal to the weight of the minimum weight vector is $ min{2d_1,d_2}$

\end{proof}
\end{thm}



\begin{thm}
Minimum distance, $d=2^{m-r}$
\begin{proof}
 Let $\V{f} \in \RM{r+1}{m}$ and $\V{g} \in \RM{r}{m}$. By theorem ~\ref{concat} .
Let $d_1 = 2^{m-r-1}$ and $d_2 = 2^{m-r}$. 
By ~\ref{general}, $d = min\{2d_1,d_2\} = 2^{m-r}$. 

The standard proof of this by induction is found everywhere - see \cite{sloane}, \cite{pless}.
\end{proof}
\end{thm}


\begin{lem}
Every codeword in $\RM{1}{m}$ (except $\V{1}, \V{0}$) has weight $2^{m-1}$
\begin{proof}
This can also be proved by using the randomization lemma for boolean functions as done in \cite{sloane}. However, if we notice that
\begin{equation*}
  \RM{1}{m} = \{\V{u}|\V{u} : \V{u} \in \RM{1}{m-1} \} \union \{(\V{u},\vec{1}+\V{u}) , \V{u} \in \RM{1}{m-1} \} 
\end{equation*}
We use induction on $m$ to complete the proof.
\end{proof}
\end{lem}

\begin{thm}
  \begin{equation*}
    \RM{m-r-1}{m} = \RM{r}{m}^{\bot}
  \end{equation*}
  \begin{proof}
    Let $\V{a} \in \RM{m-r-1}{m}$, and $\V{b} \in \RM{r}{m}$.
Then $a$ and $b$ are polynomials of degree $m-r-1$ and $r$ respectively.
$\V{ab} \in \RM{m-2}{m}$, and since $\RM{m-2}{m}$ contains all the even weight vectors, $a\cdot b \equiv 0 (mod 2) $.
Thus, $\RM{m-r-1}{m} \subset \RM{r}{m}^{\bot}$.
But by considering the dimensions we get
\begin{equation*}
  dim(\RM{m-r-1}{m}) + dim(\RM{r}{m} = 2^m = n
\end{equation*}
This implies the relation.
  \end{proof}
\end{thm}

\begin{thm}
The dual code $\RM{1}{m}^{\bot}$ is the extended binary Hamming code $H(m)$ 
  \begin{proof}
    TODO. this is important. From singa book
  \end{proof}
\end{thm}

\section {\RM{1}{3}}

\begin{equation}
\begin{array}{l|cccccccc}
1 \quad&  	 1&1&1&1&1&1&1&1 \\

v_1 \quad& 	 0&0&0&0&1&1&1&1 \\
v_2 \quad& 	 0&0&1&1&0&0&1&1 \\
v_3 \quad&	 0&1&0&1&0&1&0&1 \\
v_1+v_2 \quad&    0&0&1&1&1&1&0&0 \\
v_1+v_3 \quad&	 0&1&1&0&0&1&1&0 \\
v_2+v_3 \quad&	 0&1&0&1&1&0&1&0 \\
v_1+v2+v_3 \quad& 0&1&1&0&1&0&0&1 \\

0	\quad&	 0&0&0&0&0&0&0&0 \\
1+v_1	\quad&	 1&1&1&1&0&0&0&0 \\
1+v_2	\quad&	 1&1&0&0&1&1&0&0 \\
1+v_3	\quad&	 1&0&1&0&1&0&1&0 \\
1+v_1+v2 \quad&	 1&1&0&0&0&0&1&1 \\
1+v_1+v3 \quad&	 1&0&0&1&1&0&0&1 \\
1+v_2+v_3 \quad&	 1&0&1&0&0&1&0&1 \\
1+v_1+v_2+v_3\quad& 1&0&0&1&0&1&1&0 \\

\end{array}

\end{equation}

\section{G(2,3)}

\begin{equation}
\begin{array}{l|cccccccc}
1 \quad&         1&1&1&1&1&1&1&1 \\

v_1 \quad&       0&0&0&0&1&1&1&1 \\
v_2 \quad&       0&0&1&1&0&0&1&1 \\
v_3 \quad&       0&1&0&1&0&1&0&1 \\
v_1\cdot v_2 \quad& 0&0&0&0&0&0&1&1 \\ 
v1\cdot v_3 \quad& 0&0&0&0&0&1&0&1 \\
v2\cdot v_3 \quad& 0&0&0&1&0&0&0&1 \\

\end{array}
\end{equation}


\section{Bounds}
\label{bounds}

$A_q(n, d)$ is the largest integer $M$ such that there exists a $q$-ary $(n, M, d)$ code where $M$ is the number of codewords. A $q$-ary $(n, M, d)$ code $C$ is called optimal if $M = A_q(n, d)$. 

The \rem{} codes satisfy the \emph{Plotkin bound}.

\begin{theorem}[Plotkin Bound]
    If $C = [n,k,d]$ code, \begin{equation*}
d \leq \frac{n2^{k-1}}{2^k - 1}
\end{equation*}
  \begin{proof}
Counting in two ways:
\begin{equation*}
  D = \sum_{\V{u} \in C}{ \sum_{\V{v} \in C} {d(u,v)}
\end{equation*}
Since $d(u,v) \geq d$, $D \geq 2^k(2^k-1)d$. 

Let $D_{1i}$ denote the number of codewords with a $1$ in their $i$-th component.
\begin{equation*}
  D = \sum_{i \in n}{\sum_{j=\{0,1\}} {D_{ji}(2^k-D_{ji})}}
\end{equation*}

Since $\sum{D_{ji}} = 2^k $, we have:
\begin{equation*}
  D = n2^{2k} - \sum_{i \in n}{\sum_{j=\{0,1\}} {D_{ji}^2}}
\end{equation*}
Some manipulation yields:
\begin{equation*}
  \sum_{j=0,1}{D_{ji}^2} \geq 2^{2k-1}, \qquad i \in n
\end{equation*}
Combining the upper and the lower bounds:
\begin{equation*}
  2^k(2^k-1)d \leq D \leq n2^{2k-1}
\end{equation*}

  \end{proof}
\end{theorem}

It can be verified that $\RM{1}{m}$ codes satisfy this bound. The right hand side is:
\begin{equation*}
  \frac{2^m2^{1+m-1}}{2^{1+m}-1}
\end{equation*}
which is asymptotically equal to $d= 2^{m-1}$. Since we have hit a bound, $\RM{1}{m}$ codes are optimal.

We can also confirm this by noting that the equality holds only if the code is equidistant, and that has already been shown in \ref{equidistant}.


\subsection{Geometries}

Some properties of the \rem{} codes can be best stated using finite geometries. A comprehensive source of the relation between the code and finite geometry is ~\cite{assumus-polynomial} and ~\cite{sloane}.

\begin{lem}
  $\V{f} \in \RM{r}{m}$ is the incidence vector of the set $S$, then $\V{hf} \in \RM{r+1}{m}$ is the incidence vector of $S \cup H$
\end{lem}

\begin{thm}
  Let $f$ be a minimum weight codeword of $\RM{r}{m}$, and $f = \chi(S) $. Then $S$ is an $(m-r)$ dimensional flat in $AG(m,2)$.

  \begin{proof}[From \cite{slonae}]
Let $H$ be a hyperplane in $AG(m,2)$ and let $H'$ be the parallel hyperplane.

By the previous lemma, $S \cap H$ and $S \cap H'$ are in $\RM{r+1}{m}$ and thus contain $0$ or $\geq 2^{m-r-1}$ points. (Minimum weight , \ref{minwt}). 
But since $|S| = 2^{m-r} = |S \cap H| + |S \cap H'| $ , we have
\begin{equation*}
  |S H| = 0,2^{m-r-1}, \text{ or } 2^{m-r}
\end{equation*}

Using the rothschild-vanlint theorem \cite{rothschild}, we get that $S$ is an $(m-r)$-dimensional flat in $AG(m,2)$.
  \end{proof}
\end{thm}

The converse of this theorem also holds:
\begin{thm}
  The incidence vector of any $(m-r)$ dimensional flat in $AG(m,2)$ is a codeword in $\RM{r}{m}$.
  \begin{proof}
    Every $(m-r)$ dimensional flat in $AG(m,2)$ can be represented by $r$ linearly independent linear equations of the form      
    \begin{equation*}
      \sum_{j \in m}{a_{ij}c_j = b_i}, \qquad i\in r
    \end{equation*}
   The elements of the flats are the vectors $c=(c_0,\ldots,c_m)$ satisfying these equations. We thus need to show that $c \in \RM{r}{m}$.
   \begin{equation*}
     \sum_{j \in m}{a_{ij}c_j} + b_i +1 =1}, \qquad i\in r
   \end{equation*}
   Since we are dealing with $\F_{2}$, we can combine all the $t$ equations to get
   \begin{equation*}
     \prod_{i \in r}( \sum_{j \in m}{a_{ij}c_j} + b_i +1) = 1
   \end{equation*}
   This is because if one of the original equations does not hold then one of the right hand sides must be zero, which makes the product in the second equation become zero. Conversely if, $c$ is a solution, then each of the equations must be satisfied since the product is one. 
The last equation is also a polynomial of degree $r$, we get that $c \in \RM{r}{m}$.

Since an affine space of rank $(m-r)$ contains $2^{m-r}$ points, we get that the incidence vector is the minimum weight vector in $\RM{r}{m}$.

  \end{proof}
\end{thm}

\begin{thm}
  The number of codewords of minimum weight in $\RM{r}{m}$ is \begin{equation*}
A_2^{m-r} = 2^r \prod_{i=0}^{m-r-1}{\frac{2^{m-i} -1}{2^{m-r-i} -1}}
\end{equation*}
 
\end{thm}

A lemma that will help us in decoding:
\begin{lem}
  In an affine geometry $AG(m,2)$, each $r$-flat is constained in exactly ${2^{m-r} -1}$ flats of dimension $r+1$. These $r+1$-flats intersect pairwise exactly in the elements of the given $r$-flat.
  \begin{proof}
Let $V$ be an $r$-dimensional vector subspace of $\F_2^m$. $W=(V,w)$ is an $r+1$ dimensional space containing $V$. We can choose $w$ in $2^{r+1}-2^r$ different ways such that the same vector space $W$ is obtained. Thus there are $\frac{2^m - 2^r}{2^{r+1} - 2^r} = 2^{m-r}-1$ linear subspaces $W$ of dimension $r+1$ containing $V$. Any two of these subspaces are going to intersect exactly in $V$.

$V+x$ is a contained in $W+x$. Also the collection of all these $W+x$ flats intersect pairwise in exactly $V+x$.
  \end{proof}
\end{lem}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Weight distribution}

For the codes of order $r\geq 2$, determining the complete weight distribution is hard. (impossible).
For $r=1$, as shown \ref{r1m}, the weight of all words is $2^{m-1}$.

Considerable work has been done to establish the distribution for $\RM{2}{m}$. We state the classic results (for proofs see \cite{sloane}) : 

\cite{kasami+tokura}
\begin{equation}
  \label{eq:4}
  \text{There are at least} 2^{mr−r(r−1)} . 
\text{minimum weight codewords in } \RM{r}{m}. 
\end{equation}

For the second order codes, we have:
\begin{equation}
  \label{eq:6}
  A_i =  0 \qquad i \neq 2^{m-1}, 2^{m-1}+-2^{m-1-h} \text{ for } 0 \leq h \leq [m/2]
  A_0 = 1
  A_{2^m} = 1
  A_{ 2^{m-1}+-2^{m-1-h}} = 2^{h(h+1)}\cdot \frac{(2^m - 1)(2^{m-1}-1)\ldots (2^{m-2h+1} -1)} { (2^{2h} -1)(2^{2h-2} -1)\ldots (2^{2} -1)}
\end{equation}

\section{cyclic decoding}

In addition, it is next shown that a cyclic RM⋆ (r, m) code is simpler to decode. The 
argument is as follows. It is shown in Chapter 3 that in a cyclic code C , if (v1 , v2 , . . . , vn ) 
is a code word of C , then its cyclic shift (vn , v1 , . . . , vn 
−1 ) is also a code word of C . 
Therefore, once a particular position can be corrected with ML decoding, all the other 
positions can also be corrected with the same algorithm (or hardware circuit) by cyclically 
shifting received code words, until all n positions have been tried.

\RM{}{}

\section {Decoding}
\label{decoding}

As mentioned earlier, the \rem{} family of codes came into prominence because of the ease of decoding. In this section we present a variety of decoding algorithms. 
Several very efficient algorithms have been known specially for the \rem{} codes of the first order. 

\subsection {Majority Logic Decoding}

Majority Logic Decoding is the oldest and simplest method to decode the \rem{} codes. \cite{assmus}
Some terminology is needed for a clearer understanding:
Let $C$ be an arbitrary code. Then the \emph{parity-check} is simply a codeword in the orthogonal code $C^{\bot}$. 
Define the \emph{support-vector} of a vector $\vec{v}$ as the coordinates where it is non-zero.For example, the support-vector of $01010100$ is $(2,4,6)$.
Suppose we are given $J$ parity checks and a coordinate position, $i$, such that the intersection of all the $J$ supports is precisely $\{i\}$. The $J$ parity checks essentially `vote' for the position $i$ , and the result of the majority for the parity of $i$ is the value. 
A collection of such parity checks described above is said to be \emph{orthogonal} to $i$. If each of the coordinates of the code has a collection of $J$ parity checks orthogonal to it, then the code is capable of correcting up-to $J/2$  errors.
Another definition of orthogonality is: A set of parity check equations is called orthogonal on the $i$ coordinate if $x_i$ appears in each equation and no other $x_j$ appears more than once in the set.

\begin{Theorem} 
If there are $J$ parity checks on every co-ordinate, the code can correct $J/2$ errors.
\begin{proof}
Let $m_i$ be the message bit at coordinate $i$. $x_i$ is the received bit. 
Let $m_i=0$
Since there are $J$ orthogonal parity equations voting for $x_i$, if there are fewer than $J/2$ errors, then the result is unchanged. Therefore atleast $J/2$ equations are $0$ and thus $x_i=0$.
If $m_i=1$
Still less than $J/2$ equations are affected, and we get the right result.
\end{proof}
\end{Theorem}

Note: Ties can be broken in any way consistently. Either $0$, etc.

\begin{Theorem}
The number of errors that can be corrected by one-step majority logic decoding, $E_1$ is at most $n-1/2(d'-1)$ where $d'$ is the minimum distance of the dual code.

Proof: The dual code gives the parity check equations. 
Consider the 1st coordinate, then the parity checks orthogonal will have the form
\begin{equation*}
1  1 1 1  0 0 0 0 0
1  0 0 0  1 1 1 1 1 
\end{equation*}
Since there are $J$ orthogonal equations there will be $J$ such vectors.
Therefore by the definition of orthogonality the coordinate positions in the dual code vectors cannot overlap, hence $J \leq n-1/(d'-1) $.
We get the desired result by using the previous theorem.
\end{Theorem}


\subsection{L-step decoding}
\cite{mann}
We can extend the idea of step-decoding using orthogonal parity checks by generalizing for parity check equations to be orthogonal on a set of coordinates. 

Definition: A set of parity checks $S_1, S_2,\ldots$ is orthogonal on coordinates $i,j,..$ if the sum $x_i+x_j+\ldots$ appears in each S but no other $x_p$ appears more than once in the set.

\begin{Theorem}
The number of errors which can be corrected by an L-step majority decoding scheme, $E_l \leq n/d' - 1/2 $ 
\begin{proof}
Let the $i-th$ parity check involve $a_i$ coordinates (besides the $l$). 
Since these checks correspond to the codewords in the dual code, we have

\begin{equation}
l+a_i \geq d' 

a_i + a_j \geq d'
\end{equation}

We also have 
\begin{equation*}
S = \sum_{i=1}^{J}{a_i} \leq n-l
\end{equation*}
Thus, \begin{equation*}
Jl + S \geq Jd'

(J-1)S \geq \binom{J}{2}d'
\end{equation*}
Eliminating l and S, we get \begin{equation*}
J \leq 2n/d' -1
\end{equation*}

\end{proof}
\end{Theorem}


\subsection {Reed Decoding Algorithm}

The Reed decoding algorithm is a majority logic decoding scheme for decoding \rem{} codes of any order.

Each codeword from $\RM{r}{m}$ is an incidence vector that deﬁnes a subspace in which the nonzero coordinates of the codeword are points lying in the 
subspace. 
For example, if $\V{y} = 00001111 \in \RM{1}{3}$, then the corresponding points are $\{P_4,P_5,P_6,P_7\}$ .Their binary representation is $\{(100),(101),(110),(111)\}$ (simply the binary representation of the indices).
Based upon this geometry we can ﬁnd equations for orthogonal checksums. Our goal is to ﬁnd a set of orthogonal checksums that provide an estimate for the $k$-th order message bit $m_{i_1,i_2,ldots ,i_k}$ corresponding to the basis vectors $x_{i_1}x_{i_{2}}\ldots x_i_k$. 

Let the corresponding subspace be $S$.
Let $T$ be the complementary subspace of $S$ where $j_1j_2\ldots j_{m-k} = {1,2,ldots,m}-{i_1,i_2,\ldots,i_k}.$ 
The incidence vector of $T$ is $\V{x_j_1}\V{x_j_2}\ldots \V{x_j_{m-k}}$

Define a \emph{Translate} of a subspace,$T$ with respect to a point $\V{y}$ to be $y+T$.

In the reed decoding scheme, the checksums are the codeword coordinates specified by $T$. 



%%the subspace obtained by adding the binary representation of the point to each element of the subspace. 

\begin{them}
  If there are no errors, the message bit $m_b$ is given by 
  \begin{equation*}
    m_b = \sum_{P \in U_i}{x_P} \quad i=1,2,\ldots,2^{m-r}
  \end{equation*}
  \begin{proof}
    A codeword can be written as
    \begin{equation*}
      \V{v} = \sum_{t=i_1i_2\ldots i_s}{a_t\V{x_{i_1}}\V{x_{i_2}}\ldots \V{x_{i_s}}}
    \end{equation*}
    The sum is over all the subsets $\{i_1,i_2 ldots i_s \}$ of $\{1 \ldots m\}$ of size atmost $r$.
Summing up $\V{v}$ over all the translates gives:
\begin{equation*}
  \sum_{P \in U_i}x_P = \sum_t{a_t}\sum_{P \in U_i}{\V{x_{i_1}}\V{x_{i_2}}\ldots \V{x_{i_s}}}
\end{equation*}
Let $N(U_i,t)$ be the number of points in the intersection of $U_i$ and the subspace with the incidence vector $\V{x_{i_1}}\V{x_{i_2}}\ldots \V{x_{i_s}}$.
\begin{equation*}
  \sum_{P \in U_i}x_P = \sum_t{a_t N(U_i,t)}
\end{equation*}

Since the intersection of two subspaces is a subspace and the number of points in an affine subspace of rank $r$ is $2^r$.
Let $V$ and $W$ intersect in the subspace
\begin{equation*}
   \V{x_{j_1}}\V{x_{j_2}}\ldots \V{x_{j_{m-r}}} \V{x_{i_1}}\V{x_{i_2}}\ldots \V{x_{i_s}}
\end{equation*}



  \end{proof}
\end{them}

For a detailed algorithm refer to \cite{cooke} and \cite{lec9}

\subsection {Geometrical}

We can use the finite geometry construction of the codes to help in the decoding. 

\subsection {Hadamard Transforms}

\begin{def}
  The \emph{Real Vector} ${F(\V{v})}$ of a binary vector $\V{v}$ is a vector with $0$ replaced by $1$ and $1$ by $-1$.
\end{def}

\begin{lem}
  The Orthogonal code $\mathcal{O}_m$ with real vectors is equivalent to the Hadamard matrix $H_m$.
\begin{proof}
  Let the elements of this real-vectorized orthogonal code be $v$. 
  \begin{claim}
    \begin{equation*}
      v_i, v_j \in \RM{1}{m} \implies v_i\cdot v_j = 0
    \end{equation*}
    \begin{proof}
      We can prove using induction on the basis vectors. Since the code is a linear combination, the result follows.
    \end{proof}
  \end{claim}
 This is also the definition of the Hadamard matrix.
\end{proof}
\end{lem}

The \rem{} codes can thus be written like:
\begin{equation}
  \label{eq:3}
  \RM{1}{m} =
  \begin{pmatrix}
    H_m \\
    -H_m
  \end{pmatrix}
\end{equation}

Since the purpose of decoding is the find the codeword $\V{c}$ closest to the received vector $\V{v}$, we will want to maximize the correlation between the two, where correlation is the dot-product.

The Hadamard transform computes the correlation of the received vector $F(v)$ with each of the rows of the Hadamard matrix. Let the vector which has the largest correlation with $v$ be $c$. Then the goal is find $c$.

Also note that \begin{equation} F(\V{v}) = -F(\V{1}+\V{v}) \end{equation}. We will thus consider the absolute value of the correlation.

Because the rows of the matrix are the real-vector versions of the binary representation of the numbers from $1,\ldots 2^{m-1}$, we can determine the coordinate $a=(a_m\ldots a_1)$, where $a$ is the correlation value.
 
If the correlation is negative, then $c = \v{1}+a_iv_i$. Otherwise  $c = \v{1}+a_iv_i$.

\subsubsection{Fast Hadmard Transform}
In the Hadamard Transform, we need to compute the $n$  products  of vectors of length $n$, resulting in $O(n^2)$ operations. 
However, by using the \emph{Fast Hadamard Transform }, we can improve the number of operations to $O(nlogn)$. We use the recursive formulation of the Hadamard matrix  to prove the following theorem.  

Let $A \otimes B$ be the Kronecker product of matrices $A$ and $B$.

\begin{thm}
  \begin{equation*}
    H_{2^m} = M_{2^m}^{(1)}M_{2^m}^{(2)} \ldots M_{2^m}^{(m)}
\text{ where }
  M_{2^m}^{(i)} = I_{2^{m-i}}\otimes H_2 \otimes I_{2^{i-1}}, \qquad 1 \leq i \leq m
  \end{equation*}
  \begin{proof}
    By simple induction on m.
  \end{proof}
\end{thm}


This technique was employed in the \emph{Green Machine} \cite{green} decoder for the Mariner-9 space mission. 

For a detailed look into the construction of the decoder, refer ~\cite{sloane}.

\subsection {List Decoding}

A list decoding procedure for a code $C$ is a function which for given a $\V{u} \in F_q^n$ and $e > 0$ outputs all $v \elem C$ such that $\d(u,v) \leq e$. Alternately, given a decoding radius $T$, it should produce a list (set) of codewords which are distance less than $T$. List decoding is an old technique given by P.Elias in 1950 \cite{elias}, but has recently found significant attention.

The existence of a good, fast list decoding algorithm for the first order \rem{} codes was given by goldreich and levin \cite{goldreich}, \cite{sudan}. However they do not give a usable algorithm. Recently, Kabatiansky, Dumer and Tavernier \cite{kabatiansky}, \cite{dumer} have presented a simple list decoding algorithm for $\RM{1}{m}$ capable of correcting $n(\frac{1}{2} - \epsilon)$ errors in $O(n \epsilon^3)$. This is significantly better than the $\frac{n}{4}$ correcting capability of the Hadamard transform \ref{}, which uses $O(nlogn)$ time.



Johnson Bound
\cite{zuckerman10}
If $C$ is an [n,k,d] code with $d \geq (1/2-\epsilon)$, then
\[ \forall{u}\in F_q^n | \{v \in C : \Delta(u,v) \leq (1/2 - \sqrt{\epsilon})\}| leq 1/\epsilon
\]
Proof:
(from lec10.ps)

\subsection {RM(1,m) decoding algorithm}

Let $\V{y}$ be a received vector and $L_{\epsilon}(y) = \{f \in \RM{1}{m} : d(\V{y}, \V{f} ) \leq n(\frac{1}{2} - \epsilon) \} $ be the desired list. The algorithm works recursively by ﬁnding on the $i$-th step a list $L_\epsilon^i (y)$ of `candidates' which should (but may not) coincide with $i$-preﬁx of some $f(x1 , ..., xm) = f0 + f1x1 + . . . + fmxm ∈ Lϵ (y)$ 

The main idea is to approximate the Hamming distance between the received vector y and an arbitrary ``propagation'' of a candidate $c(i) (x1 , . . . , xm) = c1 x1 + . . . + ci xi$ by the sum of Hamming distances over all $i$-dimensional ``facets'' of the $m$-dimensional Boolean cube.

The $i$-th prefix of a boolean function is defined as an $i$-variate linear Boolean funtion:
\begin{equation*}
  c^{(i)}(x_1,x_2,ldots,x_i) = c_1x_1+ldots+c_ix_i
\end{equation*}

The algorithm essentially proceeds by considering more components of the candidates and in the process adds/removes candidates from the list.

Thus, in phase $i$, we use
\begin{equation*}
  L_{\epsilon}^{(i-1)} (y) = \{c^{(i-1)}(x_1,x_2,ldots,x_i)\}
\end{equation*}
to obtain
\begin{equation*}
  L_{\epsilon}^{(i)} (y) = \{c^{(i1)}(x_1,x_2,ldots,x_i) = c^{(i-1)}+c_ix_i\}
\end{equation*}
Note that $x_i$ can be any of the remaining $m-i$ basis vectors, and hence we get $2^{m-i}$ `extensions' of $c^{(i)}$ for each $c^{(i-1)}$. This is an exponentially growing list, and we consider how to limit it's size next.

Given a $2^m$ length vector, we consider its coordinates to be the points in an $m$-dimensional Boolean cube. Cosider an $i$-dimensional \emph{facet} of this cube as
\begin{equation*}
  S_\alpha = \{(x_1,\ldots x_i, \alpha_{i+1}\ldots \alpha_{m} \} 
\text{where} \alpha=(\alpha_{i+1}, \ldots , \alpha_m)
\end{equation*}
The prefixes $(x_1,\ldots x_i)$ run through $\F_2^i$, and we get a set of coordinates.

Thus, each facet is a particular subset of the coordinates of the vector.
Let the \emph{restriction} of a vector $\V{y}$ to a given facet $S_\alpha$ be denoted by $\V{y_\alpha}$. The restriction to a facet yields the vector values only on those coordinates.

Keeping note of the fact that we have to compute the inner product of the received vector with each of the codewords , let $\V{v_\alpha^{(i)}} \equiv \V{y_\alpha}\V{c^{(i)}}$

Define our distance function  as
\begin{equation*}
  \Delta(\V{y},\V{c^{(i)}}) = \sum_\alpha|\V{y_\alpha}\V{c^{(i)}}| = \sum_\alpha|\V{v_\alpha^{(i)}}|
\end{equation*}

The central idea of the algorithm is to calculate the $\Delta$ for each candidate $\V{c^{(i)}}$ and use this function as an upperbound .

Then in each step $i$, we use the function $\Delta(\V{y},\V{c^{(i)}})$ and construct the list of prefixes which are under the threshold
\begin{equation*}
   L_{\epsilon}^{(i)} (y) = \{c^{(i1)} : \Delta(\V{y},\V{c^{(i)}}) \geq n\epsilon \}
\end{equation*}

The final key idea is to calculate $\V{v_\alpha^{(i)}}$ recursively. 
 $S_\alpha$, the $i$-dimensional facet can be written as two $(i-1)$-dimensional subfacets
 \begin{equation*}
   S_{0,\alpha} =  \{(x_1,\ldots x_{i-1},0, \alpha_{i+1}\ldots \alpha_{m} \} 

 S_{1,\alpha} =  \{(x_1,\ldots x_{i-1},1, \alpha_{i+1}\ldots \alpha_{m} \} 
 \end{equation*}

Since $c^{(i)} = c^{(i-1)}+ c_ix_i$, we can write :
\begin{equation*}
  \V{v_\alpha^{(i)}} = \V{v_{0,\alpha}^{(i)}} + (-1)^{c_i}\V{v_{1,\alpha}^{(i-1)}}
\end{equation*}

The algorithm runs for $m$ phases.

This algorithm is similar to the fast hadamard transform described earlier \ref{fht}. The key difference is that in the fast hadamard transform, all the $2^m$ vector products are computed, albiet in an efficient manner. Here, we perform the distance-check after each phase to limit the number of candidates.

For a proof of correctness of the algorithm , see \cite{kabatiansky}. Analysis of running times and a probabilistic version is also presented. 


\subsection{general bounds}
\cite{dumer2008}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other decoding algorithms}
Besides the algorithms presented above, there exists a large number of algorithms. 
Majority logic algorithms are:
Some run in $O(n)$ time. There are hard and soft decision algorithms as well.

See \cite{fourier}, \cite{dumer2004recursive}, \cite{gopalan2008list} 

\section{Generalized \rem codes}

\section {Applications}
\label{applications}


The \rem{} codes typically find application in communication and complexity theory. In this section a few representative examples are presented. 
\subsection{Communication}

The oldest and original use. \rem{} codes were used in the 1969 Mariner-9 deep space probe to the IEEE 802.11b standard for Wireless Local Area Networks (WLANs) ~\cite{feldman}.         

\subsection{Testing Low-degree polynomials}

~\cite{lowdeg} 

Using $\RM{1}{m}$ codes to test whether a binary function is a low-degree polynomial. The functions are mapped to the \rem{} codes, and the properties are used to prove the bounds on the number of queries needed. 

\subsection{Sidelnikov cryptanalysis}

The Sidenikov public-key system, (a variant of the McEliece cryptosystem) uses \rem{} codes instead of Goppa codes because of the efficient decoding ~\cite{sidelnikov}.
A cryptanalysis attack also uses the properties of \rem{} codes to break the cryptographic code ~\cite{attack}. The uniqueness result proved earlier is a central feature in the cryptographic attack.
~\cite{correlation}
\subsection{Side Channel attacks}

~\cite{roche}

~\cite{codingtheory+blog}

\subsection{Other Applications}
~\cite{snakes} , ~\cite{OFDM}, ~\cite{}. 

\section{Conclusion}

\section {References}

\bibliographystyle{plain}      
\bibliography{report}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

\message{ !name(report.tex) !offset(-959) }
